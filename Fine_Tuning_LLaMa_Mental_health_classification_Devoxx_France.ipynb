{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nklk1-w-KHw"
   },
   "source": [
    "# üß† Fine-Tuning LLaMA for Mental Health Text Classification\n",
    "\n",
    "## üß¨ NLP Meets Mental Health | ü§ó Transformers + ü¶ô LLaMA\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Overview\n",
    "\n",
    "This project explores fine-tuning the **Meta LLaMA model** on mental health-related text to classify emotional or psychological states using **transformers**, **LoRA**, and **custom datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîñ Sections\n",
    "- ‚öôÔ∏è Setup Environment (GPU, Dependencies)\n",
    "- üóÉÔ∏è Load and Preprocess Mental Health Dataset\n",
    "- üß† Fine-Tune LLaMA with LoRA\n",
    "- üß™ Evaluate Model Performance\n",
    "- üìà Visualize and Interpret Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KRSnQvcAWUp"
   },
   "source": [
    "## ‚öôÔ∏è Setup Environment\n",
    "\n",
    "### 1Ô∏è‚É£ Installing Required Packages\n",
    "We install the necessary libraries including `transformers`, `datasets`, `peft` for LoRA fine-tuning ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers datasets evaluate accelerate pipeline bitsandbytes\n",
    "!pip install  pandas scikit-learn\n",
    "!pip install torch torchdata\n",
    "!pip install peft\n",
    "!pip install loralib\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zz-mJaDABASg"
   },
   "source": [
    "### 2Ô∏è‚É£ Importing Libraries\n",
    "We import all essential modules for fine-tuning, tokenization, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "from typing import List\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    GenerationConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import torch\n",
    "import evaluate\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    ")\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Oiih3B6C3pX"
   },
   "source": [
    "### 3Ô∏è‚É£ üîê Connecting to Hugging Face Hub\n",
    "We log in to Hugging Face to access models and upload results securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja9w4wMiDPtA"
   },
   "source": [
    "## üßπ Data Preparation\n",
    "\n",
    "### 1Ô∏è‚É£ Loading and Cleaning the Data\n",
    "We load the mental health dataset and apply basic cleaning to handle null values, formatting, and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HajarGH/sentiment-analysis-for-mental-health\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F29vDm6pDs9q"
   },
   "source": [
    "### 2Ô∏è‚É£ üè∑Ô∏è Encoding the Labels\n",
    "The categorical target labels (e.g., depression, anxiety, etc.) are encoded into numerical format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "df['label'] = label_encoder.fit_transform(df['status'])\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label mapping:\", label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-fXZrHRD3sJ"
   },
   "source": [
    "### 3Ô∏è‚É£ ‚úÇÔ∏è Splitting the Dataset\n",
    "We split the data into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuqiURBWEIOA"
   },
   "source": [
    "### 4Ô∏è‚É£ üßº Tokenization\n",
    "We tokenize the text data using the appropriate tokenizer for the LLaMA model, preparing inputs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# 5. Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"statement\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df).map(tokenize_function, batched=True)\n",
    "test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acJKT_irFDmA"
   },
   "source": [
    "## üß† Model & Training Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0k3ZyX6SFHc2"
   },
   "source": [
    "### 1Ô∏è‚É£ Load Pretrained LLaMA Model with Classification Head\n",
    "We load a pretrained LLaMA model and attach a classification head suitable for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Load model with classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_mapping),\n",
    "    problem_type=\"single_label_classification\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HItuf3ErFY7e"
   },
   "source": [
    "### 2Ô∏è‚É£ üß© Prepare for PEFT (LoRA)\n",
    "We integrate LoRA using the PEFT library to fine-tune only a small set of parameters efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Prepare for PEFT\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"  # for sequence classification\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpVSVKqbF9Ql"
   },
   "source": [
    "### 3Ô∏è‚É£ üìä Define Evaluation Metrics\n",
    "Set up accuracy, F1, precision, and recall metrics to evaluate model performance meaningfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQD1rhAwGLtP"
   },
   "source": [
    "### 4Ô∏è‚É£ üîÅ Training the Model\n",
    "Fine-tune the model on the training set using gradient accumulation and mixed-precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./Mental-health-classification\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=3,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=True,\n",
    "        fp16=True\n",
    "    ),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 10. Evaluation\n",
    "results = trainer.evaluate()\n",
    "print(f\"Final accuracy: {results['eval_accuracy']:.4f}\")\n",
    "print(f\"Final F1 score: {results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_rdYvTOIafA"
   },
   "source": [
    "### 5Ô∏è‚É£ ‚úÖ Evaluating the Model\n",
    "Run the model on the test set and compute the defined metrics to assess performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load tokenizer & base pretrained model\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Important: Load model WITHOUT PEFT for base evaluation\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 2. Run inference on the test dataset\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# 3. Compute metrics\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# 4. Display classification report\n",
    "print(classification_report(labels, preds, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load base model and tokenizer\n",
    "model_path = \"HajarGH/Mental-health-classification\"\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", num_labels=len(label_encoder.classes_))\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "# 3. Run inference\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# 4. Compute and print classification report\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "print(classification_report(labels, preds, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
